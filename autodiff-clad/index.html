<!DOCTYPE html>
<!--
  Minimal Mistakes Jekyll Theme 4.23.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
--><html lang="en" class="no-js">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Automatic Differentiation - VT space</title>
<meta name="description" content="Given a continuos function $f(x)$, how do you compute it’s derivative $f’(x_0)$  for a particular input $x_0$ computationally? Similarly, for a vector valued function $f(\vec{\boldsymbol{x}})$, how do we compute its gradient $\nabla_{\vec{\boldsymbol{x_0}}} f(\vec{\boldsymbol{x}})$? Where $\vec{\boldsymbol{x}}$ is an $n$ dimensional vector and">


  <meta name="author" content="Vaibhav Thakkar">
  
  <meta property="article:author" content="Vaibhav Thakkar">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="VT space">
<meta property="og:title" content="Automatic Differentiation">
<meta property="og:url" content="https://vaithak.github.io/autodiff-clad/">


  <meta property="og:description" content="Given a continuos function $f(x)$, how do you compute it’s derivative $f’(x_0)$  for a particular input $x_0$ computationally? Similarly, for a vector valued function $f(\vec{\boldsymbol{x}})$, how do we compute its gradient $\nabla_{\vec{\boldsymbol{x_0}}} f(\vec{\boldsymbol{x}})$? Where $\vec{\boldsymbol{x}}$ is an $n$ dimensional vector and">



  <meta property="og:image" content="https://vaithak.github.io/assets/images/llvm.png">



  <meta name="twitter:site" content="@vaithak">
  <meta name="twitter:title" content="Automatic Differentiation">
  <meta name="twitter:description" content="Given a continuos function $f(x)$, how do you compute it’s derivative $f’(x_0)$  for a particular input $x_0$ computationally? Similarly, for a vector valued function $f(\vec{\boldsymbol{x}})$, how do we compute its gradient $\nabla_{\vec{\boldsymbol{x_0}}} f(\vec{\boldsymbol{x}})$? Where $\vec{\boldsymbol{x}}$ is an $n$ dimensional vector and">
  <meta name="twitter:url" content="https://vaithak.github.io/autodiff-clad/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://vaithak.github.io/assets/images/llvm.png">
    
  

  



  <meta property="article:published_time" content="2023-05-14T00:00:00+05:30">





  

  


<link rel="canonical" href="https://vaithak.github.io/autodiff-clad/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Vaibhav Thakkar",
      "url": "https://vaithak.github.io/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="VT space Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@2.1.1/build/pseudocode.min.css">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@2.1.1/build/pseudocode.min.js"></script>

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  <script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

  <body class="layout--single">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          VT space
          
        </a>
        <ul class="visible-links">
<li class="masthead__menu-item">
              <a href="/tags/">tags</a>
            </li>
<li class="masthead__menu-item">
              <a href="/year-archive/">posts</a>
            </li>
</ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/assets/images/profile.jpg" alt="Vaibhav Thakkar" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Vaibhav Thakkar</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>Interested in working on Open source scientific softwares, AI and/or Robotics.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">India</span>
        </li>
      

      
        
          
            <li><a href="https://www.linkedin.com/in/vaithak/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
          
            <li><a href="https://vaithak.github.io" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Website</span></a></li>
          
        
          
            <li><a href="https://github.com/vaithak" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://twitter.com/vaithak" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i><span class="label">Twitter</span></a></li>
          
        
          
            <li><a href="https://www.goodreads.com/review/list/30560993-vaibhav-thakkar" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-goodreads" aria-hidden="true"></i><span class="label">goodreads</span></a></li>
          
        
      

      

      
        <li>
          <a href="mailto:vaibhav.thakkar.22.12.99@gmail.com">
            <meta itemprop="email" content="vaibhav.thakkar.22.12.99@gmail.com">
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Automatic Differentiation">
    <meta itemprop="description" content="Given a continuos function $f(x)$, how do you compute it’s derivative $f’(x_0)$  for a particular input $x_0$ computationally?Similarly, for a vector valued function $f(\vec{\boldsymbol{x}})$, how do we compute its gradient $\nabla_{\vec{\boldsymbol{x_0}}} f(\vec{\boldsymbol{x}})$?Where $\vec{\boldsymbol{x}}$ is an $n$ dimensional vector and">
    <meta itemprop="datePublished" content="2023-05-14T00:00:00+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Automatic Differentiation
</h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title">
<i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu">
<li><a href="#numerical-differentiation">Numerical Differentiation</a></li>
<li><a href="#symbolic-differentiation">Symbolic Differentiation</a></li>
<li>
<a href="#automatic-differentiation">Automatic Differentiation</a><ul>
<li><a href="#high-level-idea">High level Idea</a></li>
<li><a href="#example-computation-graph">Example Computation Graph</a></li>
<li><a href="#forward-mode-ad">Forward Mode AD</a></li>
<li><a href="#reverse-mode-ad">Reverse Mode AD</a></li>
<li>
<a href="#implementation-strategies">Implementation strategies</a><ul>
<li><a href="#overloaded-operators">Overloaded Operators</a></li>
<li><a href="#source-code-transformation">Source Code Transformation</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
          </aside>
        
        <p>Given a continuos function $f(x)$, how do you compute it’s derivative $f’(x_0)$  for a particular input $x_0$ computationally?<br>
Similarly, for a vector valued function $f(\vec{\boldsymbol{x}})$, how do we compute its gradient $\nabla_{\vec{\boldsymbol{x_0}}} f(\vec{\boldsymbol{x}})$?
Where $\vec{\boldsymbol{x}}$ is an $n$ dimensional vector and</p>

<p>$$
  \nabla f(\vec{\boldsymbol{x}}) = [ \frac{\partial f}{\partial x_0},  \frac{\partial f}{\partial x_1}, \dots \frac{\partial f}{\partial x_n}]^T
$$</p>

<p>Computing these is beneficial in many applications - for example:</p>
<ul>
  <li>Training of parameterized ML models (including Deep Neural Networks): this involves finding the optimal set of value of parameters $\boldsymbol{\hat{\theta}}$ which minimizes a particular loss function function $\mathcal{L}$ given a particular set of training examples $\boldsymbol{X}$ and corresponding labels $\boldsymbol{y}$, i.e:</li>
</ul>

<p>$$
\begin{equation}
  \boldsymbol{\hat{\theta}} = \arg_{\theta} \min \mathcal{L} (\boldsymbol{X}, \boldsymbol{y}, \boldsymbol{\theta})
\end{equation}
$$</p>

<p>To solve the above optimization problem, we use some variant of gradient descent which exploits the  property of gradient that it always points in the direction of maximum increase.<br>
The optimization algorithm has the following iterative form:</p>

<p>$$
\begin{equation}
  \boldsymbol{\hat{\theta}}_{t+1} = \boldsymbol{\hat{\theta}}_{t} - \eta \nabla_{\hat{\theta}_t}\mathcal{L} (\boldsymbol{X}, \boldsymbol{y},  \theta)
\end{equation}
$$</p>

<p>The above equation essentially states that at each time step we move a little bit in the negative direction of the loss function’s gradient at the current estimate.</p>

<h2 id="numerical-differentiation">Numerical Differentiation</h2>
<p>Well, we can start by the definition of the derivative:</p>

<p>$$
f’(x_0) = \lim_{h \rightarrow 0} \frac{f(x_0+h) - f(x_0)}{(x_0+h) - x} = \lim_{h \rightarrow 0} \frac{f(x_0+h) - f(x_0)}{h}
$$</p>

<p>Now, using the above formulation we can compute the function $f$ at very small values of $h$, for example: $10^{-5}$.</p>

<p>This seems to be a good solution for cases where $f$ is sort of a black box function and we don’t have access to its inner details/compositions. But it has a major flaw:</p>
<blockquote>
  <p>for computing gradient of a $n$ dimensional vector valued function, it requires $2n$ computations of function $f$.</p>
</blockquote>

<p>This can be particularly expensive for cases where $n$ is large, which is definitely the case for Deep Neural Networks based models.</p>

<h2 id="symbolic-differentiation">Symbolic Differentiation</h2>
<p>Treating $f$ as a black-box didn’t work out, this means we need to exploit how the function works internally - what are it’s individual smaller subfunctions for which $f$ is the composition.</p>

<p>For example: 
$$
\begin{align}
\text{if } f(x) &amp;= \log(x)\sin^2(x), \newline 
\Rightarrow f(x)  &amp;= f_1(x) \times f_2(f_3(x)) \newline
\text{where } f_1(x) &amp;= \log(x), f_2(x) = x^2, f_3(x) = \sin(x)
\end{align}
$$</p>

<p>Using the above structure, we can use the properties of derivatives like product rule and chain rule to compute the derivative of the complete function $f$ using the derivatives of $f_1, f_2, f_3$.
This way we can get an exact expression of the derivative function $f’(x)$ using a rule based engine as follows:</p>
<center>
    <img src="/assets/images/sym-diff.png" width="85%"><br>
    <em style="font-size: 0.7em"> Rules for symbolic differentiation</em>
</center>
<p><br>
This is what is used by tools like Wolfram Alpha.</p>

<ul>
  <li>This seems like a perfect solution but the question is: do we want the complete expression of the derivative? For example: if $f$ is a product of $n$ such functions, then the derivative expression will contain $\approx 2^n$ terms, some of which may be redundant computations.
One faces a similar problem when differentiating deep neural networks, which consists of composing many layers on top of each other.</li>
  <li>Although this method works for purely mathematical functions, but what about handling control flow? i.e how do we handle <em>if</em> conditions and <em>for</em> loops.</li>
</ul>

<h2 id="automatic-differentiation">Automatic Differentiation</h2>
<p>The aim is to get a procedure for computing the derivative of any general program. Hence, we come to Automatic Differentiation.</p>
<h3 id="high-level-idea">High level Idea</h3>
<p>Automatic Differentiation (or AD or autodiff) breaks down the computation into a computation graph of primitive operations (like $+, -, \times, \div$) and some primitive functions (like $\sin, \cos, \exp$) and uses chain rule and other basic rules of differentation to compute the derivatives in a procedural manner.</p>

<h3 id="example-computation-graph">Example Computation Graph</h3>
<center>
    <img src="/assets/images/comp-graph.png" width="85%"><br>
    <em style="font-size: 0.7em"> An example computation graph taken from Pytorch's blog </em>
</center>
<p><br></p>

<h3 id="forward-mode-ad">Forward Mode AD</h3>
<p>In forward mode AD, the aim is to calculate the derivative of the ouput(s) with respect to any one input variable. This is done by calculating the derivative w.r.t that input at each node, starting from the input nodes and traversing in the toplogical order all the way upto all output nodes.</p>

<p>This way we are essentially computing $\frac{\partial n}{\partial x_i}$ for every node $n$, where $x_i$ is the input node related to which we want the derivative results.</p>
<center>
    <img src="/assets/images/fwd-mode-autodiff.png" width="85%"><br>
    <em style="font-size: 0.7em"> Forward mode AD example </em>
</center>
<p><br></p>

<p>Using forward mode AD for computing gradient of a function $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ will require $n$ traversals of the computation graph, i.e once for each input variable (although, we can vectorize this whole method which will be discussed in a future post).</p>

<h3 id="reverse-mode-ad">Reverse Mode AD</h3>
<p>As the name suggests, this method is completely opposite of the fwd mode, the aim is to calculate the derivative of one ouput value with respect to all input variables. This is done by accumulating all the derivatives while going in reverse - from output nodes to input nodes.</p>
<center>
    <img src="/assets/images/rev-acc-autodiff.png" width="65%"><br>
    <em style="font-size: 0.7em"> Reverse accumulation of derivatives </em>
</center>
<p><br></p>

<p>In this method, we are essentially computing $\frac{\partial out_i}{\partial n}$ for every node $n$, where $output_i$ is the output node related to which we want the derivative results. This is done starting from the putput nodes and traversing in the reverse toplogical order all the way upto input nodes.</p>

<center>
    <img src="/assets/images/rev-mode-autodiff.png" width="85%"><br>
    <em style="font-size: 0.7em"> Reverse mode AD example </em>
</center>
<p><br></p>

<p>Using reverse mode AD for computing gradient of a function $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ will require $ms$ reverse traversals of the computation graph, i.e once for each output variable.</p>
<blockquote>
  <p>Reverse mode AD is particular helpful in cases where $m$ is very less, for example in deep neural networks the final ouput is a single scalar value of the loss function (hence $m$ = 1), that’s why deep learning libraries mainly use this mode for computing gradients w.r.t to all the parameters of the model.</p>
</blockquote>

<h3 id="implementation-strategies">Implementation strategies</h3>
<p>There are mainly two ways of implementing AD, one works at runtime by overloading the operators used in the source function, this is the implementation approach used inside python libraries like <code class="language-plaintext highlighter-rouge">autograd</code> and deep learning frameworks like <code class="language-plaintext highlighter-rouge">pytorch</code>.<br>
The other strategy is to use the AST of the source program for understanding the corresponding computational graph and build the AST for the derivative function, which is passed to the further stages of compilation to generate the code for the newly constructed AST.</p>
<h4 id="overloaded-operators">Overloaded Operators</h4>
<p>In this approach, all operations are performed on a custom defined class. This class makes sure to overload all the commonly used operators and functions for two main reasons:</p>
<ul>
  <li>When computing the expressions, it makes sure to create a new object for every results which also keeps track of the inputs and operation used to produce this node. Hence, maintaining the computational graph. This looks very similar to the basic python snippet below:
    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Value</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">parents</span><span class="o">=</span><span class="p">(),</span> <span class="n">_op</span><span class="o">=</span><span class="s">''</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="c1"># internal variables used for graph construction
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">_prev</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">parents</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">function</span> <span class="o">=</span> <span class="n">_op</span> <span class="c1"># the operation that produced this node
</span>
    <span class="k">def</span> <span class="nf">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">data</span> <span class="o">+</span> <span class="n">other</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">),</span> <span class="s">'add'</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div>    </div>
    <p>Using this, when adding two <code class="language-plaintext highlighter-rouge">Value</code> objects, a new <code class="language-plaintext highlighter-rouge">Value</code> object is created which also keeps track of it’s input nodes.</p>
  </li>
  <li>This also allows defining how gradients should be propagated for each particular operation/function (for both forward or reverse mode AD).</li>
</ul>

<center>
    <img src="/assets/images/autograd-comp-graph.png" width="85%"><br>
    <em style="font-size: 0.7em"> An example of how this might look like by overloading numpy operations (as done by autograd library).</em>
</center>
<p><br></p>

<p><em>Because the computation graph is formed dynamically at runtime, there is no need for the implementation to understand the control flow of the source program.</em></p>
<h4 id="source-code-transformation">Source Code Transformation</h4>
<p>In this approach, the source code of the program to be differentiated is traversed at compile time and a new function is automatically generated for the corresponding derivative code.
Benefit of having this approach is that the generated code can take benfit of compiler based optimizations.</p>

<p>One example project for this on which I am contributing is <a href="https://compiler-research.org/clad/">clad</a>.
It is a clang plugin which constructs the computational graph by analysing the AST of the program and allows doing both forward or reverse mode AD for computing quantities like gradient and jacobian.
For complete details on its working, you can refer to the documentation on the <a href="https://compiler-research.org/clad/">project page</a>.</p>

<center>
    <img src="/assets/images/clad.png" width="100%"><br>
    <em style="font-size: 0.7em"> Overview of clad </em>
</center>
<p><br></p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#autodiff" class="page__taxonomy-item" rel="tag">autodiff</a>
    
    </span>
  </p>




        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2023-05-14T00:00:00+05:30">May 14, 2023</time></p>


      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?via=vaithak&text=Automatic+Differentiation%20https%3A%2F%2Fvaithak.github.io%2Fautodiff-clad%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fvaithak.github.io%2Fautodiff-clad%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fvaithak.github.io%2Fautodiff-clad%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/gsoc-project-overview-part1/" class="pagination--pager" title="Counting Linear Extensions: GSoC Project Overview
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/gsoc.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/gsoc-project-overview-part1/" rel="permalink">Counting Linear Extensions: GSoC Project Overview
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          14 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">I have been selected as a student developer in Google Summer of Code under the GeomScale organization &amp; will be spending the upcoming 10 Weeks working on...</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://github.com/vaithak" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://twitter.com/vaithak" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://www.goodreads.com/review/list/30560993-vaibhav-thakkar" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-goodreads" aria-hidden="true"></i> goodreads</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">© 2023 Vaibhav Thakkar. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>







    
  <script>
    var disqus_config = function () {
      this.page.url = "https://vaithak.github.io/autodiff-clad/";  /* Replace PAGE_URL with your page's canonical URL variable */
      this.page.identifier = "/autodiff-clad"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */
    };
    (function() { /* DON'T EDIT BELOW THIS LINE */
      var d = document, s = d.createElement('script');
      s.src = 'https://https-vaithak-github-io-my-blog.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript>


  





  </body>
</html>
