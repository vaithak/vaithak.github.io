<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="https://vaithak.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://vaithak.github.io/" rel="alternate" type="text/html" /><updated>2023-12-01T23:04:08+05:30</updated><id>https://vaithak.github.io/feed.xml</id><title type="html">VT space</title><subtitle>My personal website.</subtitle><author><name>Vaibhav Thakkar</name><email>vaibhav.thakkar.22.12.99@gmail.com</email></author><entry><title type="html">Some thoughts on self-learning</title><link href="https://vaithak.github.io/self-learning/" rel="alternate" type="text/html" title="Some thoughts on self-learning" /><published>2023-12-01T00:00:00+05:30</published><updated>2023-12-01T00:00:00+05:30</updated><id>https://vaithak.github.io/self-learning</id><content type="html" xml:base="https://vaithak.github.io/self-learning/">&lt;p&gt;&lt;em&gt;Context:&lt;/em&gt; I recently had a discussion with a friend about the process of self-learning, which is very essential for scientific and engineering disciplines. During which, the point of maintaining consistency or flow state came up. I have struggled with this myself, and have had a disorganized pile of thoughts and conjectures about this for a long time. This post aims to organize them a bit.&lt;/p&gt;

&lt;p&gt;Note: I am writing this late at night, so expecting many grammatical and structural mistakes, but I don’t care about perfection here.&lt;/p&gt;

&lt;h2 id=&quot;learning-equiv-detective-work&quot;&gt;Learning $\equiv$ Detective work&lt;/h2&gt;
&lt;p&gt;There is an inherent beauty in acquiring knowledge (ज्ञान). Several great human minds have talked about the importance of this,  some notable and my favourite examples being:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The goal of mankind is knowledge . . . What man “learns” is really what he
	discovers by taking the cover off his own soul, which is a mine of infinite
	knowledge.&lt;/p&gt;

  &lt;p&gt;&lt;cite&gt;Swami Vivekanand&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Spend each day trying to be a little wiser than you were when you woke up. 
	Day by day, and at the end of the day-if you live long enough-like most people, 
	you will get out of life what you deserve.”&lt;/p&gt;

  &lt;p&gt;&lt;cite&gt;Charlie Munger&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Along with this, I believe there is an inherent flaw of not realizing this beauty and trying to gain knowledge purely to achieve some materialistic desire, for example. - learning system design for the purpose of passing a job interview, learning ML/AI because of the hype, and reading books because everyone seems to be doing it.
It is okay for such desires to spark your initial interest, but the fuel for the remaining journey should be pure curiosity. There is a reason why reading fiction (specifically detective novels) or reading about some new controversy theory on the internet is comparatively easier. There is an inherently compelling story in such cases that raises many questions within us - Why the hell did he commit the murder? What is the government hiding? …&lt;/p&gt;

&lt;p&gt;You will observe this to be true for your favourite professors or teachers. All of them began the class with “why” to take this course, and for each new topic or lecture, they started with an enquiry: Why are we studying this new technique? What was the flaw with the last topic we studied? What problem was the original inventor/discoverer trying to solve?…. Some will even let you experience the same journey through assignments or experiments. Some will let you devise your solution first, then point out its flaws - eventually leading you to think, “How the hell can this be even solved?”.&lt;/p&gt;

&lt;p&gt;During self-learning, the responsibility of asking such queries is on the learner itself, and forming this inquisitive mindset is achievable once we are aware of it. This also helps in pruning and traversing the knowledge graph of what to study. For example, there are a plethora of articles/papers and videos on Machine Learning or just &lt;a href=&quot;https://en.wikipedia.org/wiki/Large_language_model&quot;&gt;LLMs&lt;/a&gt;, and going with a mindset to understand it all is the perfect way to provide more business to the companies producing blood-pressure medicines. Instead, a better way would be to enter with queries -&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;What the hell is a LLM?
    &lt;ul&gt;
      &lt;li&gt;This may lead you to read some introductory and high level articles by amazing people like &lt;a href=&quot;https://karpathy.ai/&quot;&gt;Andrej Karpathy&lt;/a&gt; or &lt;a href=&quot;https://magazine.sebastianraschka.com/?utm_source=substack&amp;amp;utm_medium=web&amp;amp;utm_campaign=substack_profile&quot;&gt;Sebastian Raschka’s substack&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;How did they (companies like OpenAI / Anthropic / Google) made it?
    &lt;ul&gt;
      &lt;li&gt;Now if you are new to deep learning, this will lead to somewhat deeper dive. But if you have a decent understanding of machine learning and deep learning, you may ask - what changed in the recent times? Was just more data sufficient for this?&lt;/li&gt;
      &lt;li&gt;This will lead you to realizing that the &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;new architecture of transformers&lt;/a&gt; was particularly important to achieve this. So you read more about this …&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Similarly for books, reading a book because many people read it is not a good approach. Spend some time skimming the intro of the book. Does it raise enough curiosity in you that you want to delve deeper? If not, move on, maybe it is not the right time and circumstances for you to read this now.&lt;/p&gt;

&lt;p&gt;This investigative and divergent approach of learning will require more effort compared to the linear learning that we are used to in schools, but this doesn’t meaning taking structured-courses is wrong, even if you are taking a course - try to formulate a query or a question for every new thing.&lt;/p&gt;

&lt;h2 id=&quot;improving-the-system&quot;&gt;Improving the system&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;You do not rise to the level of your goals. You fall to the level of your systems.&lt;/p&gt;

  &lt;p&gt;&lt;cite&gt;James Clear, author of Atomic Habits&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Even if we have acquired a good mindset to learn something new, there are some ways in which the learning system can be improved for better consistency.&lt;/p&gt;

&lt;h4 id=&quot;iterative-improvement-after-reflection&quot;&gt;Iterative improvement after reflection&lt;/h4&gt;
&lt;p&gt;Just like any good system, premature optimization is bad for the learning system. This also means that we need to iteratively improve our system after carefully reflecting on what is not working, this is a purely subjective process.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Pain + Reflection = progress&lt;/p&gt;

  &lt;p&gt;&lt;cite&gt;Ray Dalio&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To create an analogy to the Deep Learning techniques, we need to tune many hyperparameters for the learning process. An example being the learning rate of the gradient descent algorithm. Similarly, while starting a book, it is tempting to read a lot on the first few days, but it might consume a lot of effort, thus leading to lack of motivation in the next days - and the momentum collapses. A better solution would be to start small maybe just two pages a day, let the momentum build up, get used to the system and let consistency produce better results than one-shot efforts.&lt;/p&gt;

&lt;p&gt;Also, there will some days when progress may not be possible, but consistency is not about a lot of progress every-day, but mainly about some progress for the majority of days.&lt;/p&gt;
&lt;figure&gt;
	&lt;center&gt;
		&lt;img src=&quot;/assets/images/consistency.jpeg&quot; width=&quot;35%&quot; height=&quot;auto&quot; style=&quot;border-radius: 10%&quot; /&gt;
		&lt;figcaption&gt; Taken from twitter &lt;/figcaption&gt;
	&lt;/center&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;boredom-is-good-for-you&quot;&gt;Boredom is good for you&lt;/h4&gt;
&lt;p&gt;Learning is not just done while you are focusing on it while sitting at the table. It requires connecting the dots with the existing information while you are lost in some thoughts. There are several articles and reports on why boredom is good for us. I found this &lt;a href=&quot;https://youtu.be/LKPwKFigF8U?si=tGPHcuSrKJVhU0w6&quot;&gt;video by Veritasium&lt;/a&gt; and &lt;a href=&quot;https://fs.blog/learning-baggage/&quot;&gt;this article by Farnam Street&lt;/a&gt; particularly helpful. This also means we need to resist the temptation of checking phones / social media while we are bored.&lt;/p&gt;

&lt;h4 id=&quot;make-sure-to-keep-the-balance&quot;&gt;Make sure to keep the balance&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;Medicine, law, business, engineering, these are noble pursuits and necessary to sustain life. 
But poetry, beauty, romance, love, these are what we stay alive for.&lt;/p&gt;

  &lt;p&gt;&lt;cite&gt;Robin Williams&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;While being on this journey, you may feel giving up because life is more than just acquiring knowledge 
and working on some projects. But satisfactions is not found in the extremes, it is better to maintain balance.&lt;/p&gt;

&lt;p&gt;The weights of this balance can be subjective, for some, it may be 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0.4*work + 0.2*family/friends + 0.2*exercise...&lt;/code&gt;.  The weights will also change with time, and not just the weights, the dimensions of this equation itself might change. I doubt that anyone will be able to sustain the extreme case of &lt;a href=&quot;https://en.wikipedia.org/wiki/One-hot&quot;&gt;one-hot vector&lt;/a&gt; - either completely focused on work or something else.&lt;/p&gt;

&lt;p&gt;A good thing about this vector is that the dimensions are not independent, some effort in one of them will also be effective for the others. For example, I love to play football, anytime and anywhere, and have observed that I am able to better work if I have played a game in the recent days (btw it has been more than 3 months since I last played and my work productivity is failing rapidly because of this).&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;p&gt;Life is made of star-stuff. We are the means through which the cosmos aims to understand itself.&lt;/p&gt;

  &lt;p&gt;&lt;cite&gt;Carl Sagan&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Vaibhav Thakkar</name><email>vaibhav.thakkar.22.12.99@gmail.com</email></author><category term="personal-improvement" /><summary type="html">Context: I recently had a discussion with a friend about the process of self-learning, which is very essential for scientific and engineering disciplines. During which, the point of maintaining consistency or flow state came up. I have struggled with this myself, and have had a disorganized pile of thoughts and conjectures about this for a long time. This post aims to organize them a bit.</summary></entry><entry><title type="html">Vectorized Automatic Differentiation</title><link href="https://vaithak.github.io/vectorized-autodiff-clad/" rel="alternate" type="text/html" title="Vectorized Automatic Differentiation" /><published>2023-08-22T00:00:00+05:30</published><updated>2023-08-22T00:00:00+05:30</updated><id>https://vaithak.github.io/vectorized-autodiff-clad</id><content type="html" xml:base="https://vaithak.github.io/vectorized-autodiff-clad/">&lt;p&gt;In the &lt;a href=&quot;/autodiff-clad&quot;&gt;last post&lt;/a&gt;, we discussed techniques for computing derivatives (or gradients) of arbitrary computational functions, with automatic differentiation (AD) being the most generic approach; i.e., it works for any function, even those containing control flow statements like for-loops and if-conditions.&lt;/p&gt;

&lt;p&gt;This post will discuss how we can vectorize that approach. Specifically, we will discuss vectorizing forward mode AD (although reverse mode AD can be vectorized in a similar way).&lt;/p&gt;

&lt;p&gt;The examples shared in this post are from my implementation of vectorized forward mode AD in the &lt;a href=&quot;https://github.com/vgvassilev/clad&quot;&gt;Clad project, a source transformation based AD tool&lt;/a&gt;, as part of the &lt;a href=&quot;https://summerofcode.withgoogle.com/programs/2023/projects/Ro5V6AT1&quot;&gt;Google Summer of Code program&lt;/a&gt;.&lt;/p&gt;

&lt;center&gt;
    &lt;img src=&quot;/assets/images/gsoc-cern.png&quot; width=&quot;80%&quot; /&gt;
&lt;/center&gt;

&lt;h2 id=&quot;recap-of-forward-mode-ad&quot;&gt;Recap of forward mode AD&lt;/h2&gt;
&lt;p&gt;Let’s briefly recap the basics of forward mode AD with an example, this will also help in building an intuition for the vectorized version.&lt;br /&gt;
Let’s consider a simple example:&lt;/p&gt;

&lt;p&gt;$$
\begin{align}
    f : \mathbb{R^3} &amp;amp;\rightarrow \mathbb{R} \newline
    f(x, y, z) &amp;amp;= x+y+z \newline 
    \text{  compute } &amp;amp; \frac{\partial{f}}{\partial{x}}
\end{align}
$$&lt;/p&gt;

&lt;p&gt;Steps for computing the derivating using forward mode AD,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Form the computation graph, with nodes being inputs and the operators.&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
    &lt;img src=&quot;/assets/images/autodiff-example-f.png&quot; width=&quot;80%&quot; /&gt;
&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;Computing the function is equivalent to going through this graph in forward direction.&lt;/li&gt;
  &lt;li&gt;Derivatives can also be computed in the similar fashion, difference being that each node computes $\frac{\partial{\text{ (node output) }}}{\partial{x}}$, thus propagating to $\frac{\partial{f}}{\partial{x}}$ in the end.
    &lt;ul&gt;
      &lt;li&gt;This means that each input node is initialized with a 0 or 1 value. (1 for input node $x$ as $\frac{\partial{x}}{\partial{x}} = 1$, whereas other input nodes are assigned 0 as the initial value).&lt;/li&gt;
      &lt;li&gt;At each operator node, we apply basic rules of differentiation (for example, using the sum rule at $+$ node and the product rule at $*$ node).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
    &lt;center&gt;
        &lt;img src=&quot;/assets/images/autodiff-example-f_x.png&quot; width=&quot;80%&quot; /&gt;
        &lt;figcaption&gt; Computing $\frac{\partial{f}}{\partial{x}}$ &lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;working-example&quot;&gt;Working Example&lt;/h3&gt;
&lt;p&gt;Let’s try to generate the derivative function for this example with clad:&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// Call clad to generate the derivative of f wrt x.&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_dx_dz&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;differentiate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;x&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;// Execute the generated derivative function.  &lt;/span&gt;
  &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_dx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;cm&quot;&gt;/*x=*/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;cm&quot;&gt;/*y=*/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;cm&quot;&gt;/*z=*/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure&gt;
    &lt;center&gt;
        &lt;figcaption&gt; Function generated by clad for differentiating $f$ w.r.t $x$.&lt;/figcaption&gt;
        &lt;img src=&quot;/assets/images/down-arrow.svg&quot; style=&quot;width: 10%&quot; /&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f_darg0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_d_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_d_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_d_z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_d_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_d_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_d_z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;vectorized-forward-mode-ad&quot;&gt;Vectorized forward mode AD&lt;/h2&gt;

&lt;p&gt;Now that we have seen how to compute $\frac{\partial{f}}{\partial{x}}$, how about computing $\frac{\partial{f}}{\partial{y}}$, and similarly $\frac{\partial{f}}{\partial{z}}$ ?&lt;/p&gt;
&lt;figure&gt;
    &lt;center&gt;
        &lt;img src=&quot;/assets/images/autodiff-example-f_y.png&quot; width=&quot;80%&quot; /&gt;
        &lt;figcaption&gt; Computing $\frac{\partial{f}}{\partial{y}}$ &lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;
&lt;hr /&gt;

&lt;figure&gt;
    &lt;center&gt;
        &lt;img src=&quot;/assets/images/autodiff-example-f_z.png&quot; width=&quot;80%&quot; /&gt;
        &lt;figcaption&gt; Computing $\frac{\partial{f}}{\partial{z}}$ &lt;/figcaption&gt;
    &lt;/center&gt; 
&lt;/figure&gt;

&lt;p&gt;Although, the strategy is pretty similar, it requires 3 passes for computing partial derivatives w.r.t. the 3 scalar inputs of the function. Can we combine these?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;At each node, we maintain a vector, storing the complete gradient of that node’s output w.r.t. all the input parameters , $\nabla{n} = \left[ \frac{\partial{n}}{\partial{x}}, \frac{\partial{n}}{\partial{y}}, \frac{\partial{n}}{\partial{z}} \right]$ (where $n = \text{node output}$).&lt;/li&gt;
  &lt;li&gt;All operations are now vector operations, for example, applying the sum rule will result in the addition of vectors.&lt;/li&gt;
  &lt;li&gt;Initialization for input nodes is done using &lt;a href=&quot;https://en.wikipedia.org/wiki/One-hot&quot;&gt;one-hot&lt;/a&gt; vectors.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;center&gt;
        &lt;img src=&quot;/assets/images/autodiff-example-f_grad.png&quot; /&gt;
        &lt;figcaption&gt; Computing $\nabla{f}$ &lt;/figcaption&gt;
    &lt;/center&gt; 
&lt;/figure&gt;

&lt;h3 id=&quot;defining-the-proposed-solution&quot;&gt;Defining the proposed solution&lt;/h3&gt;
&lt;p&gt;For computing the gradient of a function with $n$-dimensional input ($\in \mathbb{R^n}$) - forward mode requires n forward passes.&lt;/p&gt;

&lt;p&gt;We can do this in a single forward pass, if instead of accumulating a single scalar value of the derivative with respect to a particular node, we maintain a gradient vector at each node.&lt;/p&gt;

&lt;center&gt;
  &lt;img src=&quot;/assets/images/autodiff-forward.png&quot; style=&quot;width: 60%&quot; /&gt; $\rightarrow$
  &lt;img src=&quot;/assets/images/autodiff-vector-fwd.png&quot; style=&quot;width: 26%&quot; /&gt;
  &lt;figcaption&gt; Reference: &lt;a href=&quot;https://jnamaral.github.io/icpp20/slides/Hueckelheim_Vector.pdf&quot;&gt; jnamaral.github.io &lt;/a&gt;&lt;/figcaption&gt;
&lt;/center&gt;

&lt;h3 id=&quot;working-example-1&quot;&gt;Working example&lt;/h3&gt;
&lt;p&gt;Same example function, but now computing $\frac{\partial{f}}{\partial{x}}$ and $\frac{\partial{f}}{\partial{z}}$ together.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// Call clad to generate the derivative of f wrt x and z.&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_dx_dz&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;differentiate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector_mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;x,z&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;// Execute the generated derivative function.  &lt;/span&gt;
  &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dz&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;f_dx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;cm&quot;&gt;/*x=*/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;cm&quot;&gt;/*y=*/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;cm&quot;&gt;/*z=*/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure&gt;
    &lt;center&gt;
        &lt;figcaption&gt; Function generated by clad for differentiating $f$ w.r.t. $x$ and $z$ using vector mode.&lt;/figcaption&gt;
        &lt;img src=&quot;/assets/images/down-arrow.svg&quot; style=&quot;width: 10%&quot; /&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f_dvec_0_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_d_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_dz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;clad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_vec_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;clad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_vec_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;clad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_vec_z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;clad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_vec_ret&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_vec_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_vec_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_vec_z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_d_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_vec_ret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_d_z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_vec_ret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that, now in a single forward pass, we can compute derivate w.r.t. multiple parameters together.&lt;br /&gt;
The main change in the interface is using the option &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;clad::opts::vector_mode&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;benefits&quot;&gt;Benefits&lt;/h2&gt;
&lt;p&gt;Now that each node requires computing a vector, this requires more memory (and more time goes into these memory allocation calls), which must be offset by some improvement in the computing efficiency.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;This can prevent the re-computation of some expensive functions, which would have executed in a non-vectorized version due
to multiple forward passes.&lt;/li&gt;
  &lt;li&gt;This approach can take advantage of the hardware’s vectorization and parallelization capabilities (using SIMD techniques).&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
    &lt;img src=&quot;/assets/images/vector-operations.png&quot; style=&quot;width: 80%&quot; /&gt;
&lt;/center&gt;

&lt;h2 id=&quot;progress-in-clad&quot;&gt;Progress in Clad&lt;/h2&gt;
&lt;p&gt;The complete progress report of this project along with the missing features and future goals can be &lt;a href=&quot;https://gist.github.com/vaithak/82125fa9618c81741dcecb88f0e76d4b&quot;&gt;seen here&lt;/a&gt;, and a basic documentation of using this vectorized mode is &lt;a href=&quot;https://clad.readthedocs.io/en/latest/user/UsingVectorMode.html&quot;&gt;available here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The following is a more complex example of differentiation functions consisting of array parameters using the vectorized forward mode AD implementation in Clad.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// A function fo weighted sum of array elements.&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;weighted_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure&gt;
    &lt;center&gt;
        &lt;figcaption&gt; Using Clad&apos;s vector mode for differentiating w.r.t. `arr` and `weights` parameters.&lt;/figcaption&gt;
        &lt;img src=&quot;/assets/images/down-arrow.svg&quot; style=&quot;width: 10%&quot; /&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;weighted_sum_dvec_0_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_ref&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_d_arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_ref&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_d_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// Calculating total number of independent variables.&lt;/span&gt;
  &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;indepVarCount&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_d_arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_d_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;// Initializing each row with a one-hot vector is equivalent to initializing an&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// identity matrix (possible shifted and rectangular) for complete array.&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;clad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_d_vector_arr&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;identity_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_d_arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;indepVarCount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0UL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;clad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_d_vector_weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;identity_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_d_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;indepVarCount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_d_arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;());&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;clad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_d_vector_n&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indepVarCount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;clad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_d_vector_res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indepVarCount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
  &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

  &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;clad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_d_vector_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indepVarCount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;_d_vector_res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_d_vector_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_d_vector_arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;clad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_d_vector_return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indepVarCount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_d_vector_res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_d_arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_d_vector_return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0UL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_d_arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;());&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_d_weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_d_vector_return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_d_arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_d_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;());&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Vaibhav Thakkar</name><email>vaibhav.thakkar.22.12.99@gmail.com</email></author><category term="autodiff" /><summary type="html">In the last post, we discussed techniques for computing derivatives (or gradients) of arbitrary computational functions, with automatic differentiation (AD) being the most generic approach; i.e., it works for any function, even those containing control flow statements like for-loops and if-conditions.</summary></entry><entry><title type="html">Automatic Differentiation</title><link href="https://vaithak.github.io/autodiff-clad/" rel="alternate" type="text/html" title="Automatic Differentiation" /><published>2023-05-14T00:00:00+05:30</published><updated>2023-05-14T00:00:00+05:30</updated><id>https://vaithak.github.io/autodiff-clad</id><content type="html" xml:base="https://vaithak.github.io/autodiff-clad/">&lt;p&gt;Given a continuos function $f(x)$, how do you compute it’s derivative $f’(x_0)$  for a particular input $x_0$ computationally?&lt;br /&gt;
Similarly, for a vector valued function $f(\vec{\boldsymbol{x}})$, how do we compute its gradient $\nabla_{\vec{\boldsymbol{x_0}}} f(\vec{\boldsymbol{x}})$?
Where $\vec{\boldsymbol{x}}$ is an $n$ dimensional vector and&lt;/p&gt;

&lt;p&gt;$$
  \nabla f(\vec{\boldsymbol{x}}) = [ \frac{\partial f}{\partial x_0},  \frac{\partial f}{\partial x_1}, \dots \frac{\partial f}{\partial x_n}]^T
$$&lt;/p&gt;

&lt;p&gt;Computing these is beneficial in many applications - for example:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Training of parameterized ML models (including Deep Neural Networks): this involves finding the optimal set of value of parameters $\boldsymbol{\hat{\theta}}$ which minimizes a particular loss function function $\mathcal{L}$ given a particular set of training examples $\boldsymbol{X}$ and corresponding labels $\boldsymbol{y}$, i.e:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$
\begin{equation}
  \boldsymbol{\hat{\theta}} = \arg_{\theta} \min \mathcal{L} (\boldsymbol{X}, \boldsymbol{y}, \boldsymbol{\theta})
\end{equation}
$$&lt;/p&gt;

&lt;p&gt;To solve the above optimization problem, we use some variant of gradient descent which exploits the  property of gradient that it always points in the direction of maximum increase.&lt;br /&gt;
The optimization algorithm has the following iterative form:&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
  \boldsymbol{\hat{\theta}}_{t+1} = \boldsymbol{\hat{\theta}}_{t} - \eta \nabla_{\hat{\theta}_t}\mathcal{L} (\boldsymbol{X}, \boldsymbol{y},  \theta)
\end{equation}
$$&lt;/p&gt;

&lt;p&gt;The above equation essentially states that at each time step we move a little bit in the negative direction of the loss function’s gradient at the current estimate.&lt;/p&gt;

&lt;h2 id=&quot;numerical-differentiation&quot;&gt;Numerical Differentiation&lt;/h2&gt;
&lt;p&gt;Well, we can start by the definition of the derivative:&lt;/p&gt;

&lt;p&gt;$$
f’(x_0) = \lim_{h \rightarrow 0} \frac{f(x_0+h) - f(x_0)}{(x_0+h) - x} = \lim_{h \rightarrow 0} \frac{f(x_0+h) - f(x_0)}{h}
$$&lt;/p&gt;

&lt;p&gt;Now, using the above formulation we can compute the function $f$ at very small values of $h$, for example: $10^{-5}$.&lt;/p&gt;

&lt;p&gt;This seems to be a good solution for cases where $f$ is sort of a black box function and we don’t have access to its inner details/compositions. But it has a major flaw:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;for computing gradient of a $n$ dimensional vector valued function, it requires $2n$ computations of function $f$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This can be particularly expensive for cases where $n$ is large, which is definitely the case for Deep Neural Networks based models.&lt;/p&gt;

&lt;h2 id=&quot;symbolic-differentiation&quot;&gt;Symbolic Differentiation&lt;/h2&gt;
&lt;p&gt;Treating $f$ as a black-box didn’t work out, this means we need to exploit how the function works internally - what are it’s individual smaller subfunctions for which $f$ is the composition.&lt;/p&gt;

&lt;p&gt;For example: 
$$
\begin{align}
\text{if } f(x) &amp;amp;= \log(x)\sin^2(x), \newline 
\Rightarrow f(x)  &amp;amp;= f_1(x) \times f_2(f_3(x)) \newline
\text{where } f_1(x) &amp;amp;= \log(x), f_2(x) = x^2, f_3(x) = \sin(x)
\end{align}
$$&lt;/p&gt;

&lt;p&gt;Using the above structure, we can use the properties of derivatives like product rule and chain rule to compute the derivative of the complete function $f$ using the derivatives of $f_1, f_2, f_3$.
This way we can get an exact expression of the derivative function $f’(x)$ using a rule based engine as follows:&lt;/p&gt;
&lt;center&gt;
    &lt;img src=&quot;/assets/images/sym-diff.png&quot; width=&quot;85%&quot; /&gt;&lt;br /&gt;
    &lt;em style=&quot;font-size: 0.7em&quot;&gt; Rules for symbolic differentiation&lt;/em&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
This is what is used by tools like Wolfram Alpha.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;This seems like a perfect solution but the question is: do we want the complete expression of the derivative? For example: if $f$ is a product of $n$ such functions, then the derivative expression will contain $\approx 2^n$ terms, some of which may be redundant computations.
One faces a similar problem when differentiating deep neural networks, which consists of composing many layers on top of each other.&lt;/li&gt;
  &lt;li&gt;Although this method works for purely mathematical functions, but what about handling control flow? i.e how do we handle &lt;em&gt;if&lt;/em&gt; conditions and &lt;em&gt;for&lt;/em&gt; loops.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;automatic-differentiation&quot;&gt;Automatic Differentiation&lt;/h2&gt;
&lt;p&gt;The aim is to get a procedure for computing the derivative of any general program. Hence, we come to Automatic Differentiation.&lt;/p&gt;
&lt;h3 id=&quot;high-level-idea&quot;&gt;High level Idea&lt;/h3&gt;
&lt;p&gt;Automatic Differentiation (or AD or autodiff) breaks down the computation into a computation graph of primitive operations (like $+, -, \times, \div$) and some primitive functions (like $\sin, \cos, \exp$) and uses chain rule and other basic rules of differentation to compute the derivatives in a procedural manner.&lt;/p&gt;

&lt;h3 id=&quot;example-computation-graph&quot;&gt;Example Computation Graph&lt;/h3&gt;
&lt;center&gt;
    &lt;img src=&quot;/assets/images/comp-graph.png&quot; width=&quot;85%&quot; /&gt;&lt;br /&gt;
    &lt;em style=&quot;font-size: 0.7em&quot;&gt; An example computation graph taken from Pytorch&apos;s blog &lt;/em&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;forward-mode-ad&quot;&gt;Forward Mode AD&lt;/h3&gt;
&lt;p&gt;In forward mode AD, the aim is to calculate the derivative of the ouput(s) with respect to any one input variable. This is done by calculating the derivative w.r.t that input at each node, starting from the input nodes and traversing in the toplogical order all the way upto all output nodes.&lt;/p&gt;

&lt;p&gt;This way we are essentially computing $\frac{\partial n}{\partial x_i}$ for every node $n$, where $x_i$ is the input node related to which we want the derivative results.&lt;/p&gt;
&lt;center&gt;
    &lt;img src=&quot;/assets/images/fwd-mode-autodiff.png&quot; width=&quot;85%&quot; /&gt;&lt;br /&gt;
    &lt;em style=&quot;font-size: 0.7em&quot;&gt; Forward mode AD example &lt;/em&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Using forward mode AD for computing gradient of a function $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ will require $n$ traversals of the computation graph, i.e once for each input variable (although, we can vectorize this whole method which will be discussed in a future post).&lt;/p&gt;

&lt;h3 id=&quot;reverse-mode-ad&quot;&gt;Reverse Mode AD&lt;/h3&gt;
&lt;p&gt;As the name suggests, this method is completely opposite of the fwd mode, the aim is to calculate the derivative of one ouput value with respect to all input variables. This is done by accumulating all the derivatives while going in reverse - from output nodes to input nodes.&lt;/p&gt;
&lt;center&gt;
    &lt;img src=&quot;/assets/images/rev-acc-autodiff.png&quot; width=&quot;65%&quot; /&gt;&lt;br /&gt;
    &lt;em style=&quot;font-size: 0.7em&quot;&gt; Reverse accumulation of derivatives &lt;/em&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In this method, we are essentially computing $\frac{\partial out_i}{\partial n}$ for every node $n$, where $output_i$ is the output node related to which we want the derivative results. This is done starting from the putput nodes and traversing in the reverse toplogical order all the way upto input nodes.&lt;/p&gt;

&lt;center&gt;
    &lt;img src=&quot;/assets/images/rev-mode-autodiff.png&quot; width=&quot;85%&quot; /&gt;&lt;br /&gt;
    &lt;em style=&quot;font-size: 0.7em&quot;&gt; Reverse mode AD example &lt;/em&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Using reverse mode AD for computing gradient of a function $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ will require $ms$ reverse traversals of the computation graph, i.e once for each output variable.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Reverse mode AD is particular helpful in cases where $m$ is very less, for example in deep neural networks the final ouput is a single scalar value of the loss function (hence $m$ = 1), that’s why deep learning libraries mainly use this mode for computing gradients w.r.t to all the parameters of the model.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;implementation-strategies&quot;&gt;Implementation strategies&lt;/h3&gt;
&lt;p&gt;There are mainly two ways of implementing AD, one works at runtime by overloading the operators used in the source function, this is the implementation approach used inside python libraries like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;autograd&lt;/code&gt; and deep learning frameworks like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pytorch&lt;/code&gt;.&lt;br /&gt;
The other strategy is to use the AST of the source program for understanding the corresponding computational graph and build the AST for the derivative function, which is passed to the further stages of compilation to generate the code for the newly constructed AST.&lt;/p&gt;
&lt;h4 id=&quot;overloaded-operators&quot;&gt;Overloaded Operators&lt;/h4&gt;
&lt;p&gt;In this approach, all operations are performed on a custom defined class. This class makes sure to overload all the commonly used operators and functions for two main reasons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;When computing the expressions, it makes sure to create a new object for every results which also keeps track of the inputs and operation used to produce this node. Hence, maintaining the computational graph. This looks very similar to the basic python snippet below:
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parents&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# internal variables used for graph construction
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_prev&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parents&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_op&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# the operation that produced this node
&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__add__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;other&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;other&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;other&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;add&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;Using this, when adding two &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Value&lt;/code&gt; objects, a new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Value&lt;/code&gt; object is created which also keeps track of it’s input nodes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;This also allows defining how gradients should be propagated for each particular operation/function (for both forward or reverse mode AD).&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;
    &lt;img src=&quot;/assets/images/autograd-comp-graph.png&quot; width=&quot;85%&quot; /&gt;&lt;br /&gt;
    &lt;em style=&quot;font-size: 0.7em&quot;&gt; An example of how this might look like by overloading numpy operations (as done by autograd library).&lt;/em&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Because the computation graph is formed dynamically at runtime, there is no need for the implementation to understand the control flow of the source program.&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&quot;source-code-transformation&quot;&gt;Source Code Transformation&lt;/h4&gt;
&lt;p&gt;In this approach, the source code of the program to be differentiated is traversed at compile time and a new function is automatically generated for the corresponding derivative code.
Benefit of having this approach is that the generated code can take benfit of compiler based optimizations.&lt;/p&gt;

&lt;p&gt;One example project for this on which I am contributing is &lt;a href=&quot;https://compiler-research.org/clad/&quot;&gt;clad&lt;/a&gt;.
It is a clang plugin which constructs the computational graph by analysing the AST of the program and allows doing both forward or reverse mode AD for computing quantities like gradient and jacobian.
For complete details on its working, you can refer to the documentation on the &lt;a href=&quot;https://compiler-research.org/clad/&quot;&gt;project page&lt;/a&gt;.&lt;/p&gt;

&lt;center&gt;
    &lt;img src=&quot;/assets/images/clad.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
    &lt;em style=&quot;font-size: 0.7em&quot;&gt; Overview of clad &lt;/em&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Vaibhav Thakkar</name><email>vaibhav.thakkar.22.12.99@gmail.com</email></author><category term="autodiff" /><summary type="html">Given a continuos function $f(x)$, how do you compute it’s derivative $f’(x_0)$ for a particular input $x_0$ computationally? Similarly, for a vector valued function $f(\vec{\boldsymbol{x}})$, how do we compute its gradient $\nabla_{\vec{\boldsymbol{x_0}}} f(\vec{\boldsymbol{x}})$? Where $\vec{\boldsymbol{x}}$ is an $n$ dimensional vector and</summary></entry><entry><title type="html">Counting Linear Extensions: GSoC Project Overview</title><link href="https://vaithak.github.io/gsoc-project-overview-part1/" rel="alternate" type="text/html" title="Counting Linear Extensions: GSoC Project Overview" /><published>2021-10-04T00:00:00+05:30</published><updated>2021-10-04T00:00:00+05:30</updated><id>https://vaithak.github.io/gsoc-project-overview-part1</id><content type="html" xml:base="https://vaithak.github.io/gsoc-project-overview-part1/">&lt;p&gt;I have been selected as a student developer in &lt;a href=&quot;https://summerofcode.withgoogle.com/&quot;&gt;Google Summer of Code&lt;/a&gt; under the &lt;a href=&quot;https://geomscale.github.io/&quot;&gt;GeomScale organization&lt;/a&gt; &amp;amp; will be spending the upcoming 10 Weeks working on the project of “Counting Linear Extensions”. This post will be for providing details regarding my project, mainly problem formulation and proposed solution.&lt;/p&gt;

&lt;center&gt;
    &lt;img src=&quot;/assets/images/gsoc.png&quot; width=&quot;40%&quot; height=&quot;200&quot; /&gt; + &lt;img src=&quot;/assets/images/geomscale.png&quot; width=&quot;40%&quot; height=&quot;200&quot; /&gt;
&lt;/center&gt;
&lt;p&gt;&lt;a href=&quot;https://summerofcode.withgoogle.com/projects/#6649856422051840&quot;&gt;Project link&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;about-the-organization&quot;&gt;About the Organization&lt;/h2&gt;
&lt;p&gt;From the webpage of the organization:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“GeomScale is a research and development project that delivers open source code for state-of-the-art algorithms at the intersection of data science, optimization, geometric, and statistical computing”.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I have always been interested in the field of applied maths and this project immediately caught my attention.
Currently, the organization works on mainly two open-source libraries, one of which is &lt;a href=&quot;https://github.com/GeomScale/volume_approximation&quot;&gt;volesti&lt;/a&gt;, which is a C++ library with R and python interfaces, this library contains several algorithms for high-dimensional sampling and volume approximation. The other one is the recently introduced &lt;a href=&quot;https://github.com/GeomScale/dingo&quot;&gt;dingo&lt;/a&gt;, which is a python package for analyzing metabolic networks.&lt;/p&gt;

&lt;h2 id=&quot;introduction-to-my-project&quot;&gt;Introduction to my project&lt;/h2&gt;
&lt;p&gt;In brief, my project is to extend the functionality of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;volesti&lt;/code&gt; library, in particular,
implementing the functionality of “Counting Linear Extensions of a poset (partially ordered set)”.
This problem can be converted to computing volume of a convex polytope, for which &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;volesti&lt;/code&gt; contains state-of-the-art algorithms.&lt;/p&gt;

&lt;p&gt;Exact problem description is as following:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The problem of counting the linear extensions of a given partial order consists of finding (counting) all the possible ways that we can extend the partial order to a total order that preserves the original partial order.
It is an important problem with various applications in Artificial Intelligence and Machine Learning, for example in partial-order plans and in learning graphical models. It is a well known #P-complete problem; the fastest known algorithms are based on dynamic programming and require exponential time and space.
Nevertheless, if we only need an approximate counting attained with a probability of success, then the problem admits a polynomial-time solution for all posets using Markov chain Monte Carlo (MCMC) methods.
This project aims to implement new randomized methods to approximate the number of linear extensions based on volume approximation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;mathematical-details-of-the-project&quot;&gt;Mathematical details of the project&lt;/h2&gt;
&lt;p&gt;First, I will start by introducing the terms used in the above section, because once all the terms are clear, the aim of the project will become crystal clear (hopefully).&lt;/p&gt;

&lt;h3 id=&quot;poset&quot;&gt;Poset&lt;/h3&gt;
&lt;p&gt;The term “poset” is short for “partially ordered set”, that is, a set whose elements are ordered but not all pairs of elements are required to be comparable in the order.&lt;/p&gt;

&lt;p&gt;I will try to explain with an example:
Let’s say you have marks of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n=5&lt;/code&gt; students in two exams, the marks of a student (out of 10)
is given as a pair, let the marks of the students be as follows:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;student-id&lt;/th&gt;
      &lt;th&gt;marks&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$S_1$&lt;/td&gt;
      &lt;td&gt;(5, 8)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$S_2$&lt;/td&gt;
      &lt;td&gt;(7, 4)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$S_3$&lt;/td&gt;
      &lt;td&gt;(8, 3)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$S_4$&lt;/td&gt;
      &lt;td&gt;(9, 4)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$S_5$&lt;/td&gt;
      &lt;td&gt;(4, 2)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Now, let’s say your task is to find the topper of the class (yup, as always, creating an unnecessary competition in a learning environment). Now, there may exist many personal opinions on how to rank them, like take the sum (or mean) of two score, take the minimum of two scores etc. But, all these are subjective and can not be generalized to any pair of exams.
The main point is that there exist a pair of students, which cannot be compared.
for ex. $S_1$ and $S_3$ cannot be compared, similarly $S_1$ and $S_2$ cannot be compared, but there are some pairs which can be compared, like $S_1$ and $S_5$, as $S_1$ scored higher marks in both the subjects.&lt;/p&gt;

&lt;p&gt;There exists many such examples, for ex. when you are deciding which laptop to buy, you may consider several features for comparing them, like: price, RAM size, screen size, processor, company etc.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For representing the order between elements, we use the $\leq$ symbol (seems intuitive),
so we can say that $S_5 \leq S_1$, $S_3 \leq S_4$ and so on.&lt;/li&gt;
  &lt;li&gt;We say that elements $a, b$ of a poset are comparable, if either $a \leq b$ or $b \leq a$.&lt;/li&gt;
  &lt;li&gt;Although the symbol is of $\leq$, but the binary relation can be of any sort,
formally, a partial order is any binary relation that is reflexive (each element is comparable to itself), antisymmetric (no two different elements precede each other), and transitive.
&lt;a href=&quot;https://en.wikipedia.org/wiki/Partially_ordered_set&quot;&gt;Complete definition can be obtained here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;representing-poset-as-a-directed-acyclic-graph-dag&quot;&gt;Representing poset as a Directed Acyclic Graph (DAG)&lt;/h3&gt;
&lt;p&gt;One graphical way of representing the poset is by using a directed graph. &lt;br /&gt;
This is done by representing each element of the poset by a node and there will be a directed edge between every pair of nodes $a \rightarrow b$ for which $a \leq b$.&lt;/p&gt;
&lt;center&gt;
    &lt;img src=&quot;/assets/images/poset-dag-self-loops.png&quot; width=&quot;70%&quot; /&gt;&lt;br /&gt;
    &lt;em style=&quot;font-size: 0.7em&quot;&gt; Directed (not yet acyclic) Graph representing the example poset &lt;/em&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;As can be easily seen in the above graph, it is not yet acyclic because of the self loops.  &lt;br /&gt;
This can be solved easily, as we know that for every element $a$ in the poset, $a \leq a$ will always be true, hence we can simply remove the self loops
for a more compact representation. &lt;br /&gt;
This results in the following graph:&lt;/p&gt;
&lt;center&gt;
    &lt;img src=&quot;/assets/images/poset-dag.png&quot; width=&quot;55%&quot; /&gt;&lt;br /&gt;
    &lt;em style=&quot;font-size: 0.7em&quot;&gt; DAG representation of the example poset &lt;/em&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Now, although the above graph is already a DAG, but if you observe carefully, it can be made even more compact. &lt;br /&gt;
This can be done by utilizing the transitive property of the partial order relations, i.e&lt;/p&gt;

&lt;p&gt;$$
  a \leq b, b \leq c \Rightarrow a \leq c
$$&lt;/p&gt;

&lt;p&gt;This means that in the above graph, when there is already an edge from $S_5 \rightarrow S_3$ and then $S_3 \rightarrow S_4$, then the edge $S_5 \rightarrow S_4$
is not giving us any extra information, this means it is implicit and can be removed.  &lt;br /&gt;
This is known as finding the transitive reduction of a DAG, you can read more about it from &lt;a href=&quot;https://en.wikipedia.org/wiki/Transitive_reduction&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;center&gt;
    &lt;img src=&quot;/assets/images/poset-dag-transitive.png&quot; width=&quot;55%&quot; /&gt;&lt;br /&gt;
    &lt;em style=&quot;font-size: 0.7em&quot;&gt; Transitive Reduction of the above DAG &lt;/em&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;linear-extensions-of-a-poset&quot;&gt;Linear extensions of a poset&lt;/h3&gt;
&lt;p&gt;Let’s say we want to assign ranks to the students in our example, but didn’t we just discuss that there is ambiguity in that :thinking: ??&lt;/p&gt;

&lt;p&gt;Still, let’s say we have received a strict order from the principal of this school to assign ranks to the students, now you being a
teacher with good ethics decides to assign ranks with a condition, if a student $b$ has better marks than student $a$ in both exams,
i.e $a \leq b$ in the poset of the students, then definitely $b$ will get a better rank than $a$.&lt;/p&gt;

&lt;p&gt;The following are 2 example rankings that you can assign to the students (the rightmost being rank 1 and leftmost being rank 5):&lt;/p&gt;

&lt;p&gt;$$
\begin{align}
    &amp;amp; S_5 \leq S_2 \leq S_3 \leq S_4 \leq S_1  \newline
    &amp;amp; S_5 \leq S_1 \leq S_3 \leq S_2 \leq S_4
\end{align}
$$&lt;/p&gt;

&lt;p&gt;The above example rankings are examples of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Total_order&quot;&gt;total order&lt;/a&gt;, the only additional property of a &lt;em&gt;“total order or a linear order”&lt;/em&gt;
compared to a &lt;em&gt;“partial order”&lt;/em&gt; is that any two elements are comparable.&lt;/p&gt;

&lt;p&gt;Finally coming to definition of a linear extension:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;A linear extension of a partial order is a total order or a linear order that is compatible with the partial order.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the above definition, compatible means that if $a \leq b$ in the partial order, $\Rightarrow a \leq b$ in the extended total order.&lt;/p&gt;

&lt;p&gt;A genuine question that may arise in your mind , what is so linear about it? :confused:
&lt;em style=&quot;font-size: 0.8em&quot;&gt; Hint: think about how will the DAG representation of a totally ordered set look like after transitive reduction. &lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;For fun: Prove that the number of linear extensions of a poset is equal to the number of topological orderings of it’s corresponding DAG.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;naive-recursive-algorithm-for-counting-linear-extensions&quot;&gt;Naive recursive algorithm for Counting Linear Extensions&lt;/h3&gt;
&lt;p&gt;Now that we know what a linear extension is, let’s devise a basic algorithm for counting the exact number of linear extensions.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Even before thinking about an algorithm, let’s think about the scale of this number? For this, let’s take the worst case example, i.e a set of $n$ elements
with no order between any pair of elements, it is easy to see that there are $n!$ number of linear extensions for this set. Thus, we see that the count of
linear extensions can be very high ($n! \propto \sqrt{n} \left(\frac{n}{e}\right)^n$).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One way can be to enumerate all possible linear extensions, we gain insights from the above example of ranking the students, the high level view of the
algorithm can be described as follows:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;High level algorithm:&lt;/em&gt;&lt;/strong&gt;
Enumerate over all elements which can be ranked at the last of the $n$ elements, these are called &lt;a href=&quot;https://en.wikipedia.org/wiki/Maximal_and_minimal_elements&quot;&gt;minimal elements&lt;/a&gt; of the poset, nodes that are not greater than any other element of the poset, i.e a node $s$ is a minimal element if $\nexists$ any element $m$ s.t $m \leq s$.
For each minimal element, place it at the end and then recurse for the remaining $n-1$ elements.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The above algorithm is essentially enumerating over all possible linear extensions, so it’s time complexity will be $O(n!)$. Although, we can use
memoization to avoid recomputing the subproblems (or use dynamic programming for bottom up approach), still the number of subproblems will be exponential in the worst case
$\Big[ {n\choose 1} + {n\choose 2} + \dots =  O(2^n) \Big]$.&lt;/li&gt;
  &lt;li&gt;Although, the above algorithm is exponential, but the problem of &lt;a href=&quot;https://dl.acm.org/doi/10.1145/103418.103441&quot;&gt;Counting Linear Extensions is in fact #P-complete&lt;/a&gt;,
i.e it is in &lt;a href=&quot;https://en.wikipedia.org/wiki/♯P&quot;&gt;#P&lt;/a&gt; and atleast as hard as all other problems in the time complexity class #P.
(#P complexity class contains the counting versions of all the decision problems in the NP complexity class)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;font-size: 0.6em&quot;&gt;
$$
\begin{align}
  \text{COUNT-LIN-EXT}(P) &amp;amp;= \sum_{x \in min(P)} \text{COUNT-LIN-EXT}(P \setminus x), \newline
  \text{where } min(P) &amp;amp; \text{ is the set of minimal elements of the poset P} \newline
\end{align} 
$$
&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;geometric-interpretation&quot;&gt;Geometric Interpretation&lt;/h2&gt;
&lt;p&gt;For a geometrical representation of our problem, we will consider an n-element poset in n-dimensional space. Also, to make the representation bounded, we will restrict the space inside the unit cube ([0,1]&lt;sup&gt;n&lt;/sup&gt;). &lt;br /&gt;
The order relations can be interpreted as half spaces, thus the object formed has flat sides, hence it is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Polytope&quot;&gt;polytope&lt;/a&gt; (essentially an extension of polygons in high dimensions). &lt;br /&gt;
This special type of polytope formed for representing a poset is called an &lt;a href=&quot;https://en.wikipedia.org/wiki/Order_polytope&quot;&gt;order-polytope&lt;/a&gt;.&lt;/p&gt;
&lt;center&gt;
    &lt;img src=&quot;/assets/images/order-polytope.png&quot; width=&quot;95%&quot; /&gt;&lt;br /&gt;
    &lt;em style=&quot;font-size: 0.7em&quot;&gt; Geometric interpretation of an example poset as an order polytope.&lt;/em&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;relation-to-volume-computation&quot;&gt;Relation to volume computation&lt;/h2&gt;
&lt;p&gt;The following results will make a beautiful connection between counting linear extensions of a poset to the volume of its corresponding order polytope.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;RoI(P) = Union of all linear extensions of P
    &lt;ul&gt;
      &lt;li&gt;$\{x_1, x_2, x_3 \mid x_2 \leq x_3\} = \{x_1 \leq x_2 \leq x_3\} \cup \{x_2 \leq x_1 \leq x_3\} \cup \{x_2 \leq x_3 \leq x_1\}$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$Volume (RoI (P))$ = Sum of volumes of Linear Extensions of P
    &lt;ul&gt;
      &lt;li&gt;using the above the result and the fact that intersection of two linear extensions will have zero volume.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Volume of every linear extension of P = $\frac{1}{n!}$ ; $n = $ number of elements (3 in the example)
    &lt;ul&gt;
      &lt;li&gt;can be seen by taking P = set with no relations in the second result and using symmetry.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$Volume (RoI (P)) \times\ n!$ = Number of linear extensions of P
    &lt;ul&gt;
      &lt;li&gt;by combining the second and third result.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We have converted our problem of counting linear extensions into the problem of computing volume of polytopes in high dimensions. &lt;br /&gt;
As already stated, the problem of counting linear extensions is provably hard, and the above problem reduction takes polynomial time, this means that the problem of computing volume of polytopes in high dimensions is also provably hard.&lt;/p&gt;

&lt;h2 id=&quot;approximate-volume-computation-in-high-dimensions&quot;&gt;(approximate) Volume computation in high dimensions&lt;/h2&gt;
&lt;p&gt;This is where &lt;a href=&quot;https://github.com/GeomScale/volesti&quot;&gt;GeomScale’s volesti&lt;/a&gt; library comes in - it contains implementations for efficiently approximating volumes of high dimensional convex objects, it does so by utilizing Multiphase Markov Chain Monte Carlo (MCMC) methods. You can read more about it from the documentations and paper mentioned in their README.&lt;/p&gt;

&lt;p&gt;The images below provides a high level overview of these techniques:&lt;/p&gt;
&lt;center&gt;
    &lt;img src=&quot;/assets/images/monte-carlo.png&quot; width=&quot;95%&quot; /&gt;&lt;br /&gt;
    &lt;em style=&quot;font-size: 0.7em&quot;&gt; Monte Carlo method.&lt;/em&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;
    &lt;img src=&quot;/assets/images/multiphase-monte-carlo.png&quot; width=&quot;95%&quot; /&gt;&lt;br /&gt;
    &lt;em style=&quot;font-size: 0.7em&quot;&gt; Multiphase Monte Carlo.&lt;/em&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;
    &lt;img src=&quot;/assets/images/multiphase-mcmc.png&quot; width=&quot;95%&quot; /&gt;&lt;br /&gt;
    &lt;em style=&quot;font-size: 0.7em&quot;&gt; Markov Chain Monte Carlo.&lt;/em&gt;
&lt;/center&gt;</content><author><name>Vaibhav Thakkar</name><email>vaibhav.thakkar.22.12.99@gmail.com</email></author><category term="gsoc" /><summary type="html">I have been selected as a student developer in Google Summer of Code under the GeomScale organization &amp;amp; will be spending the upcoming 10 Weeks working on the project of “Counting Linear Extensions”. This post will be for providing details regarding my project, mainly problem formulation and proposed solution.</summary></entry></feed>